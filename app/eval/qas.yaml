- q: "Can you walk me through your academic path and how it shaped your move into data science and AI?"
  a: "I began in neuroscience, analyzing MRI and EEG data, which introduced me to signal processing and statistical modeling. Those skills naturally led into machine learning applications, and over time I shifted toward applied data science and AI. My academic path gave me a rigorous foundation in quantitative thinking that I carry into industry work."

- q: "How did your EEG and neuroscience research prepare you for applied ML work?"
  a: "Working with EEG required cleaning noisy signals and extracting meaningful features, which is directly analogous to feature engineering in ML. It also trained me to think critically about experimental design and evaluation. That experience translates well into designing robust ML pipelines."

- q: "Tell me about a project where you combined signal processing techniques with machine learning."
  a: "In my postdoc, I applied time-frequency and event-related analyses to EEG data to measure neural activation patterns linked with cognitive states. I used a classification model to predict cognitive state depending on frequency patterns immediately prior to a visual stimulus. The model correctly classified participants' cognitive states more than 80% of the time."

- q: "You’ve published scientific papers — how has that experience influenced how you communicate technical results today?"
  a: "Publishing trained me to be precise, concise, and transparent about methods and limitations. I bring that same approach when communicating results to technical and non-technical audiences in industry. It helps me frame findings clearly and build trust with stakeholders."

- q: "How have you used embeddings in real projects?"
  a: "I’ve used text embeddings to build semantic search and retrieval systems, including a RAG app that grounded answers in my own documents. Embeddings helped link unstructured text to meaningful outputs. They were also valuable for clustering and topic discovery."

- q: "Can you share an example of building or improving a retrieval-augmented generation (RAG) system?"
  a: "I built a RAG application that indexed my resume, CV, and project documents with FAISS. The system retrieved context snippets and used them to generate accurate, first-person interview answers. This improved relevance and ensured answers stayed grounded in real material."

- q: "Walk me through how you've evaluated a product you've worked on"
  a: "I start with forming a hypothesis about the product's impact, then quickly experiment and validate through data analysis, such as measuring lift impacts or user engagement. For example, at Butter, I evaluated the Payment Insights dashboard by analyzing customer feedback and usage metrics, which helped me refine the product and demonstrate its value to clients. This iterative approach ensures data-driven decision-making and continuous improvement."

- q: "How do you ensure models you build are both technically sound and aligned with business needs?"
  a: "I ensure models are technically sound by applying rigorous validation, feature engineering, and testing, while aligning with business needs through continuous stakeholder communication, understanding key metrics, and iterating based on feedback and performance insights. This approach guarantees that models deliver actionable, reliable results that support strategic decision-making."

- q: "Tell me about a time when you had to simplify technical findings for a non-technical audience."
  a: "During my work at Butter, I developed a Payment Insights dashboard that visualized complex payment funnel data for clients like Classpass and Masterclass. I simplified the technical details by focusing on key metrics and actionable insights, enabling clients to understand trends without technical jargon, which strengthened our customer relationships.."

- q: "Which of your projects do you consider the most innovative, and why?"
  a: "The RAG-based interview assistant was highly innovative because it combined personal documents, retrieval, and LLMs into a functional agent. It demonstrated how to tailor AI for individual use cases. The novelty was in making it both accurate and personal."

- q: "How have you incorporated external research (like published papers) into your own applied work?"
  a: "I regularly consult papers for algorithms or evaluation methods. For example, I integrated recent approaches to embeddings into a semantic search pipeline. Using academic research ensures I’m applying state-of-the-art techniques responsibly."

- q: "Tell me about a time you improved an ML pipeline or tool that wasn’t working as expected."
  a: "I diagnosed bottlenecks in a model deployment pipeline that caused unacceptable latency. By profiling steps and optimizing preprocessing, I reduced response times significantly. The improvements made the pipeline viable for production use."

- q: "What project in your portfolio best shows your ability to link theory to practice?"
  a: "My work on developing a chatbot system at Acxiom best demonstrates my ability to link theory to practice, as I applied NLP techniques like intent classification and rule-based systems to solve a real business problem—enabling sales teams to query large customer data assets naturally. This project exemplifies how I translate theoretical NLP concepts into practical, impactful solutions."

- q: "How has your interdisciplinary background (science + data + product) given you an edge in your work?"
  a: "My interdisciplinary background, combining cognitive neuroscience, data science, and product development, allows me to approach problems holistically—understanding complex data, designing innovative solutions, and aligning them with business goals. This foundation enables me to translate scientific insights into practical, scalable products that meet user needs and drive measurable impact."

- q: "Describe a project where latency or efficiency was a key requirement."
  a: "At MileIQ, I built an A/B test evaluation pipeline for drive processing algorithm improvements, focusing on optimizing route detection accuracy and drive distance metrics. I curated datasets, compared algorithms, and visualized key metrics to ensure rapid, reliable testing, which improved the efficiency of deployment cycles and decision-making."

- q: "Can you share an example where you evaluated trade-offs between model complexity and deployment constraints?"
  a: "During my work on the Eaze LTV model, I used a simple OLS linear model based on 12-month cohorts to extrapolate to 24 months, balancing model complexity with the need for timely updates and interpretability. I then productionized the model to update monthly, ensuring it met deployment constraints while providing actionable insights."

- q: "How do you think your prior academic collaborations prepared you for working with cross-functional product teams?"
  a: "Academic work trained me to collaborate with people from different disciplines, from psychologists to engineers. That experience maps directly to product teams where communication and alignment are essential."

- q: "Tell me about a project you designed that had measurable business impact."
  a: "At Rocket Money, I proposed and implemented an experimental optimization to the billing algorithm, which resulted in over $600,000 in annual savings and a direct bottom-line impact. I also built a churn prediction model using XGBoost, which provided forward-looking insights into user engagement and helped inform strategic product decisions."

- q: "Which personal or research project most excites you, and what does it say about your goals?"
  a: "The project that excites me most is the development of retrieval-augmented generative AI systems, as it reflects my goal to create human-centered, conversational interfaces that are safe, reliable, and scalable. This work demonstrates my commitment to advancing natural language interaction and my desire to pioneer innovative methods in AI productization."

- q: "How have you handled situations where the data available wasn’t ideal for the problem?"
  a: "I’ve used data augmentation, proxy variables, or combined multiple sources to compensate. For example, in EEG work I developed preprocessing to reduce noise. The key is to be transparent about limitations while still delivering value."

- q: "Tell me about a project in your CV where you had to balance experimentation with deadlines."
  a: "At Butter, I led the development of the Payment Insights dashboard, starting with a hypothesis and rapid prototyping, then collaborating with engineering and design teams to productionize it within tight deadlines. I relied on iterative experiments to guide feature prioritization, ensuring we delivered a high-impact product on schedule that strengthened customer relationships."

- q: "Have you done any work related to inference priming or cognitive neuroscience methods that you think translates into AI work?"
  a: "Yes, I studied inference priming in EEG experiments. The way context influences interpretation is very similar to how embeddings shape LLM responses. That background informs how I think about context in AI systems."

- q: "Where do you see your unique background (academic + applied + product) taking you in the AI field over the next five years?"
  a: "I see myself leading applied AI projects that balance cutting-edge research with product value. My interdisciplinary background positions me to bridge teams and ensure innovations are impactful. My goal is to help shape responsible, human-centered AI."
