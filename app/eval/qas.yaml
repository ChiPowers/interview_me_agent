# qas.yaml – gold questions with answers

questions:

* q: "Can you walk me through your academic path and how it shaped your move into data science and AI?"
  a: "I began in neuroscience, analyzing EEG data, which introduced me to signal processing and statistical modeling. Those skills naturally led into machine learning applications, and over time I shifted toward applied data science and AI. My academic path gave me a rigorous foundation in quantitative thinking that I carry into industry work."
* q: "How did your EEG and neuroscience research prepare you for applied ML work?"
  a: "Working with EEG required cleaning noisy signals and extracting meaningful features, which is directly analogous to feature engineering in ML. It also trained me to think critically about experimental design and evaluation. That experience translates well into designing robust ML pipelines."
* q: "Tell me about a project where you combined signal processing techniques with machine learning."
  a: "In my postdoc, I applied time-frequency analyses to EEG data and then trained classifiers to predict cognitive states. The project combined Fourier transforms with supervised learning to link neural signals to behavior. It showed me how domain knowledge and ML can reinforce each other."
* q: "You’ve published scientific papers — how has that experience influenced how you communicate technical results today?"
  a: "Publishing trained me to be precise, concise, and transparent about methods and limitations. I bring that same approach when communicating results to technical and non-technical audiences in industry. It helps me frame findings clearly and build trust with stakeholders."
* q: "Describe one of the projects in your CV that best demonstrates your ability to connect research to business impact."
  a: "I built a recommendation model for an online platform that improved engagement metrics significantly. The project moved from prototyping to production and showed measurable business gains. It reflects my ability to translate research into applied value."
* q: "What was the most challenging project you led, and how did you overcome obstacles to deliver results?"
  a: "A complex ML pipeline was underperforming due to noisy input data and latency constraints. I re-architected the preprocessing, added caching, and simplified the model. These changes balanced accuracy with efficiency and allowed us to deploy successfully."
* q: "How have you used embeddings in real projects?"
  a: "I’ve used text embeddings to build semantic search and retrieval systems, including a RAG app that grounded answers in my own documents. Embeddings helped link unstructured text to meaningful outputs. They were also valuable for clustering and topic discovery."
* q: "Can you share an example of building or improving a retrieval-augmented generation (RAG) system?"
  a: "I built a RAG application that indexed my resume, CV, and project documents with FAISS. The system retrieved context snippets and used them to generate accurate, first-person interview answers. This improved relevance and ensured answers stayed grounded in real material."
* q: "Walk me through how you designed evaluations for an ML system you’ve worked on."
  a: "For one model, I created a golden dataset of expected Q\&A pairs to evaluate precision and recall. I also included user-centered metrics like helpfulness and latency. This evaluation framework helped iterate quickly and track progress over time."
* q: "How do you ensure models you build are both technically sound and aligned with business needs?"
  a: "I start by clarifying the business objectives and success metrics. Then I design experiments and evaluations that align with those outcomes. This ensures the model is not only accurate but also impactful for the business."
* q: "Tell me about a time when you had to simplify technical findings for a non-technical audience."
  a: "I once explained a model’s false positives using an analogy to weather forecasts. By focusing on the concept of trade-offs, the stakeholders understood the key point without needing technical detail. It built alignment around decision-making."
* q: "Which of your projects do you consider the most innovative, and why?"
  a: "The RAG-based interview assistant was highly innovative because it combined personal documents, retrieval, and LLMs into a functional agent. It demonstrated how to tailor AI for individual use cases. The novelty was in making it both accurate and personal."
* q: "How have you incorporated external research (like published papers) into your own applied work?"
  a: "I regularly consult papers for algorithms or evaluation methods. For example, I integrated recent approaches to embeddings into a semantic search pipeline. Using academic research ensures I’m applying state-of-the-art techniques responsibly."
* q: "Tell me about a time you improved an ML pipeline or tool that wasn’t working as expected."
  a: "I diagnosed bottlenecks in a model deployment pipeline that caused unacceptable latency. By profiling steps and optimizing preprocessing, I reduced response times significantly. The improvements made the pipeline viable for production use."
* q: "What project in your portfolio best shows your ability to link theory to practice?"
  a: "My work with EEG showed theoretical grounding in neuroscience and signal processing, but I extended it by building ML models that predicted cognitive states. That bridge from theory to practice has guided many of my applied projects since."
* q: "How has your interdisciplinary background (science + data + product) given you an edge in your work?"
  a: "It helps me see problems from multiple angles. I can dive deep into data, but also step back and think about user experience and product impact. That combination lets me build solutions that are both technically rigorous and practical."
* q: "Describe a project where latency or efficiency was a key requirement."
  a: "In one RAG project, latency was capped at 5–10 seconds. I optimized retrieval with FAISS and cached frequent queries to meet those limits. The work balanced user experience with technical feasibility."
* q: "Can you share an example where you evaluated trade-offs between model complexity and deployment constraints?"
  a: "I compared a deep model with a simpler logistic regression for a classification task. While the deep model had slightly higher accuracy, it was too slow. I chose the simpler model, which met latency requirements and was easier to maintain."
* q: "How do you think your prior academic collaborations prepared you for working with cross-functional product teams?"
  a: "Academic work trained me to collaborate with people from different disciplines, from psychologists to engineers. That experience maps directly to product teams where communication and alignment are essential."
* q: "Tell me about a project you designed that had measurable business impact."
  a: "I developed an ML-driven recommendation pipeline that improved click-through rates significantly. The result was a measurable uptick in revenue. It demonstrated how data science could tie directly to business KPIs."
* q: "Which personal or research project most excites you, and what does it say about your goals?"
  a: "The interview assistant project excites me because it blends personal expression with cutting-edge AI. It reflects my goal of making AI more human-centered and useful for individual growth."
* q: "How have you handled situations where the data available wasn’t ideal for the problem?"
  a: "I’ve used data augmentation, proxy variables, or combined multiple sources to compensate. For example, in EEG work I developed preprocessing to reduce noise. The key is to be transparent about limitations while still delivering value."
* q: "Tell me about a project in your CV where you had to balance experimentation with deadlines."
  a: "I worked on a product launch where timelines were fixed. I ran rapid experiments but locked down the pipeline once performance was good enough. That balance allowed us to ship on time without compromising quality."
* q: "Have you done any work related to inference priming or cognitive neuroscience methods that you think translates into AI work?"
  a: "Yes, I studied inference priming in EEG experiments. The way context influences interpretation is very similar to how embeddings shape LLM responses. That background informs how I think about context in AI systems."
* q: "Where do you see your unique background (academic + applied + product) taking you in the AI field over the next five years?"
  a: "I see myself leading applied AI projects that balance cutting-edge research with product value. My interdisciplinary background positions me to bridge teams and ensure innovations are impactful. My goal is to help shape responsible, human-centered AI."
